{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nhttps://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\\nhttps://towardsdatascience.com/machine-learning-word-embedding-sentiment-classification-using-keras-b83c28087456\\nhttp://ruder.io/deep-learning-nlp-best-practices/\\nhttps://skymind.ai/wiki/word2vec\\nhttps://medium.com/@ppasumarthi_69210/word-embeddings-in-keras-be6bb3092831\\nhttps://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526\\nhttps://medium.com/@dcameronsteinke/tf-idf-vs-word-embedding-a-comparison-and-code-tutorial-5ba341379ab0\\nhttps://medium.com/@b.terryjack/nlp-everything-about-word-embeddings-9ea21f51ccfe\\nhttps://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795\\n'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 1
    }
   ],
   "source": [
    "'''\n",
    "https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "https://towardsdatascience.com/machine-learning-word-embedding-sentiment-classification-using-keras-b83c28087456\n",
    "http://ruder.io/deep-learning-nlp-best-practices/\n",
    "https://skymind.ai/wiki/word2vec\n",
    "https://medium.com/@ppasumarthi_69210/word-embeddings-in-keras-be6bb3092831\n",
    "https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526\n",
    "https://medium.com/@dcameronsteinke/tf-idf-vs-word-embedding-a-comparison-and-code-tutorial-5ba341379ab0\n",
    "https://medium.com/@b.terryjack/nlp-everything-about-word-embeddings-9ea21f51ccfe\n",
    "https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/konpyro/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /home/konpyro/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /home/konpyro/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package rslp to /home/konpyro/nltk_data...\n[nltk_data]   Package rslp is already up-to-date!\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 2
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('rslp')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# X: input, Y:output\n",
    "X = []\n",
    "Y = []\n",
    "with open('MoodyLyricsExtended4Q.csv') as file:\n",
    "    reader = csv.reader(file, delimiter = ',')\n",
    "    count = 0\n",
    "    for row in reader:\n",
    "        if count > 0:\n",
    "            X.append(row[4])\n",
    "            Y.append(row[3])\n",
    "        count = count + 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#Y = np.concatenate(y_train, y_test)\n",
    "y_t = np.zeros((len(Y), 4))\n",
    "for i  in range(len(Y)):\n",
    "    if Y[i] == 'happy':\n",
    "        y_t[i][:] = [1, 0, 0, 0]\n",
    "    elif Y[i] == 'angry':\n",
    "        y_t[i][:] = [0, 1, 0, 0]\n",
    "    elif Y[i] == 'sad':\n",
    "        y_t[i][:] = [0, 0, 1, 0]\n",
    "    elif Y[i] == 'relaxed':\n",
    "        y_t[i][:] = [0, 0, 0, 1]\n",
    "Y = y_t"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "total_lyrics = x_train + x_test\n",
    "t.fit_on_texts(total_lyrics)\n",
    "\n",
    "max_length = max([len(s.split()) for s in total_lyrics])\n",
    "vocab_size = len(t.word_index) + 1\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "x_train_tokens = t.texts_to_sequences(x_train)\n",
    "x_test_tokens = t.texts_to_sequences(x_test)\n",
    "\n",
    "x_train_pad = pad_sequences(x_train_tokens, padding='post')\n",
    "x_test_pad = pad_sequences(x_test_tokens, padding='post')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lyric_lines = list()\n",
    "lines = X\n",
    "\n",
    "for line in lines:\n",
    "    tokens = word_tokenize(line)\n",
    "\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    lyric_lines.append(words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Found 16375 unique tokens.\n",
      "Shape of lyric tensor: (18115, 408)\nShape of sentiment tensor; (18115, 4)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(lyric_lines)\n",
    "sequences = t.texts_to_sequences(lyric_lines)\n",
    "\n",
    "word_index = t.word_index\n",
    "print('Found %s unique tokens.' %len(word_index))\n",
    "\n",
    "review_pad = pad_sequences(sequences, maxlen=max_length)\n",
    "sentiment = np.asarray(Y)\n",
    "print('Shape of lyric tensor:', review_pad.shape)\n",
    "print('Shape of sentiment tensor;', sentiment.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Found 400000 word vectors.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('', 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "word_index = t.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "\"'\\nfilename = 'lyric_embedding_word2vec'\\nmodel.wv.save(filename)\\n\\nimport os\\n\\nembeddings_index = {}\\nf = open(os.path.join('', 'lyric_embedding_word2vec.txt'), encoding='latin-1')\\nfor line in f:\\n  print(line)\\n  values = line.split()\\n  word = values[0]\\n  coefs = np.asarray(values[1:])\\n  embeddings_index[word] = coefs\\nf.close()\\n\""
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 12
    }
   ],
   "source": [
    "''''\n",
    "filename = 'lyric_embedding_word2vec'\n",
    "model.wv.save(filename)\n",
    "\n",
    "import os\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('', 'lyric_embedding_word2vec.txt'), encoding='latin-1')\n",
    "for line in f:\n",
    "  print(line)\n",
    "  values = line.split()\n",
    "  word = values[0]\n",
    "  coefs = np.asarray(values[1:])\n",
    "  embeddings_index[word] = coefs\n",
    "f.close()\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Biuld model...\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from keras.initializers import Constant\n",
    "\n",
    "print('Biuld model...')\n",
    "model = Sequential()\n",
    "\n",
    "model.add(embedding_layer)\n",
    "#model.add(LSTM(32, dropout=0.2, return_sequences=True, recurrent_dropout=0.2))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.33\n",
    "\n",
    "indices = np.arange(review_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "review_pad = review_pad[indices]\n",
    "sentiment = sentiment[indices]\n",
    "\n",
    "x_train_pad, x_test_pad, y_train, y_test = train_test_split(review_pad, sentiment, test_size=0.33)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Shape of x_train_pad tensor: (12137, 408)\nShape of y_train tensor: (12137, 4)\nShape of x_test_pad tensor: (5978, 408)\nShape of y_test tensor: (5978, 4)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print('Shape of x_train_pad tensor:', x_train_pad.shape)\n",
    "print('Shape of y_train tensor:', y_train.shape)\n",
    "print('Shape of x_test_pad tensor:', x_test_pad.shape)\n",
    "print('Shape of y_test tensor:', y_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Train model...\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print('Train model...')\n",
    "\n",
    "model.fit(x_train_pad, y_train, epochs=20, validation_data=(x_test_pad, y_test), verbose=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(x_test_pad, y_test)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "LSTM(32),\n",
    "split: 0.33,\n",
    "epochs: 5,\n",
    "dropout=r_dropout = 0.2,\n",
    "-> **loss: 1.2, acc: 0.46**\n",
    "\n",
    "LSTM(64),\n",
    "split: 0.33,\n",
    "epochs: 5,\n",
    "dropout=r_dropout = 0.2,\n",
    "-> **loss: 1.18, acc: 0.49**\n",
    "\n",
    "LSTM(64),\n",
    "split: 0.33,\n",
    "epochs: 10,\n",
    "dropout=r_dropout = 0.2,\n",
    "loss_fun = kullback_leibler_divergence\n",
    "-> **loss: 1.19, acc: 0.48**\n",
    "\n",
    "LSTM(64),\n",
    "split: 0.33,\n",
    "epochs: 5,\n",
    "dropout=r_dropout = 0.1,\n",
    "-> **loss: 1.2, acc: 0.45**\n",
    "\n",
    "LSTM(64),\n",
    "split: 0.33,\n",
    "epochs: 25,\n",
    "dropout=r_dropout = 0.2,\n",
    "-> **loss: 1.22, acc: 0.48**\n",
    "\n",
    "LSTM(128),\n",
    "split: 0.33,\n",
    "epochs: 5,\n",
    "dropout=r_dropout = 0.2,\n",
    "-> **loss: 1.2, acc: 0.41**\n",
    "\n",
    "LSTM(32),\n",
    "LSTM(32),\n",
    "split: 0.33,\n",
    "epochs: 5,\n",
    "dropout=r_dropout = 0.2,\n",
    "-> **loss: 1.18, acc: 0.45**\n",
    "\n",
    "LSTM(64),\n",
    "LSTM(64),\n",
    "split: 0.33,\n",
    "epochs: 5,\n",
    "dropout=r_dropout = 0.2,\n",
    "-> **loss: 1.24, acc: 0.42**\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nLSTM(128),\\nsplit: 0.33,\\nepochs: 5,\\nloss: 1.2, acc: 0.41\\n------------\\n\\n'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 18
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}