{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "print(device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "https://towardsdatascience.com/machine-learning-word-embedding-sentiment-classification-using-keras-b83c28087456\n",
    "http://ruder.io/deep-learning-nlp-best-practices/\n",
    "https://skymind.ai/wiki/word2vec\n",
    "https://medium.com/@ppasumarthi_69210/word-embeddings-in-keras-be6bb3092831\n",
    "https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526\n",
    "https://medium.com/@dcameronsteinke/tf-idf-vs-word-embedding-a-comparison-and-code-tutorial-5ba341379ab0\n",
    "https://medium.com/@b.terryjack/nlp-everything-about-word-embeddings-9ea21f51ccfe\n",
    "https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/konpyro/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /home/konpyro/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /home/konpyro/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package rslp to /home/konpyro/nltk_data...\n[nltk_data]   Package rslp is already up-to-date!\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 2
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('rslp')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "1829\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# X: input, Y:output\n",
    "X1 = []\n",
    "Y1 = []\n",
    "X2 = []\n",
    "Y2 = []\n",
    "X = []\n",
    "Y = []\n",
    "with open('MoodyLyricsFull4Q.csv') as file:\n",
    "    reader = csv.reader(file, delimiter = ',')\n",
    "    count = 0\n",
    "    for row in reader:\n",
    "        if count > 0:\n",
    "            X1.append(row[4])\n",
    "            Y1.append(row[3])\n",
    "        count = count + 1\n",
    "\n",
    "with open('MoodyLyricsClear4Q.csv') as file:\n",
    "    reader = csv.reader(file, delimiter = ',')\n",
    "    count = 0\n",
    "    for row in reader:\n",
    "        if count > 0:\n",
    "            X2.append(row[4])\n",
    "            Y2.append(row[3])\n",
    "        count = count + 1\n",
    "for i in range(len(X1)):\n",
    "    if X1[i] == X2[i] and X1[i] != '' and X1[i] != ' ':\n",
    "        X.append(X1[i])\n",
    "        Y.append((Y1[i]))\n",
    "        \n",
    "print(len(X)) "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#Y = np.concatenate(y_train, y_test)\n",
    "y_t = np.zeros((len(Y), 2))\n",
    "for i  in range(len(Y)):\n",
    "    if Y[i] == 'happy':\n",
    "        y_t[i][:] = [1, 1]\n",
    "    elif Y[i] == 'angry':\n",
    "        y_t[i][:] = [1, -1]\n",
    "    elif Y[i] == 'sad':\n",
    "        y_t[i][:] = [-1, -1]\n",
    "    elif Y[i] == 'relaxed':\n",
    "        y_t[i][:] = [ -1, 1]\n",
    "Y = y_t"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "total_lyrics = x_train + x_test\n",
    "t.fit_on_texts(total_lyrics)\n",
    "\n",
    "max_length = max([len(s.split()) for s in total_lyrics])\n",
    "vocab_size = len(t.word_index) + 1\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "x_train_tokens = t.texts_to_sequences(x_train)\n",
    "x_test_tokens = t.texts_to_sequences(x_test)\n",
    "\n",
    "x_train_pad = pad_sequences(x_train_tokens, padding='post')\n",
    "x_test_pad = pad_sequences(x_test_tokens, padding='post')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lyric_lines = list()\n",
    "lines = X\n",
    "\n",
    "for line in lines:\n",
    "    tokens = word_tokenize(line)\n",
    "\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    lyric_lines.append(words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Found 16379 unique tokens.\nShape of lyric tensor: (1829, 1015)\nShape of sentiment tensor; (1829, 2)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(lyric_lines)\n",
    "sequences = t.texts_to_sequences(lyric_lines)\n",
    "\n",
    "word_index = t.word_index\n",
    "print('Found %s unique tokens.' %len(word_index))\n",
    "\n",
    "review_pad = pad_sequences(sequences, maxlen=max_length)\n",
    "sentiment = np.asarray(Y)\n",
    "print('Shape of lyric tensor:', review_pad.shape)\n",
    "print('Shape of sentiment tensor;', sentiment.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Found 400000 word vectors.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('', 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "word_index = t.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "\"'\\nfilename = 'lyric_embedding_word2vec'\\nmodel.wv.save(filename)\\n\\nimport os\\n\\nembeddings_index = {}\\nf = open(os.path.join('', 'lyric_embedding_word2vec.txt'), encoding='latin-1')\\nfor line in f:\\n  print(line)\\n  values = line.split()\\n  word = values[0]\\n  coefs = np.asarray(values[1:])\\n  embeddings_index[word] = coefs\\nf.close()\\n\""
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 12
    }
   ],
   "source": [
    "''''\n",
    "filename = 'lyric_embedding_word2vec'\n",
    "model.wv.save(filename)\n",
    "\n",
    "import os\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('', 'lyric_embedding_word2vec.txt'), encoding='latin-1')\n",
    "for line in f:\n",
    "  print(line)\n",
    "  values = line.split()\n",
    "  word = values[0]\n",
    "  coefs = np.asarray(values[1:])\n",
    "  embeddings_index[word] = coefs\n",
    "f.close()\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Biuld model...\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from keras.initializers import Constant\n",
    "\n",
    "print('Biuld model...')\n",
    "model = Sequential()\n",
    "\n",
    "model.add(embedding_layer)\n",
    "#model.add(LSTM(64, dropout=0.2, return_sequences=True, recurrent_dropout=0.2))\n",
    "model.add(LSTM(32, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(2, activation='tanh'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='categorical_hinge',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.33\n",
    "\n",
    "indices = np.arange(review_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "review_pad = review_pad[indices]\n",
    "sentiment = sentiment[indices]\n",
    "\n",
    "x_train_pad, x_test_pad, y_train, y_test = train_test_split(review_pad, sentiment, test_size=0.33)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Shape of x_train_pad tensor: (1225, 1015)\nShape of y_train tensor: (1225, 2)\nShape of x_test_pad tensor: (604, 1015)\nShape of y_test tensor: (604, 2)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print('Shape of x_train_pad tensor:', x_train_pad.shape)\n",
    "print('Shape of y_train tensor:', y_train.shape)\n",
    "print('Shape of x_test_pad tensor:', x_test_pad.shape)\n",
    "print('Shape of y_test tensor:', y_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Train model...\n",
      "Train on 1225 samples, validate on 604 samples\nEpoch 1/5\n",
      " - 21s - loss: 0.9311 - accuracy: 0.5094 - val_loss: 0.8750 - val_accuracy: 0.5281\n",
      "Epoch 2/5\n",
      " - 20s - loss: 0.8939 - accuracy: 0.5216 - val_loss: 0.8596 - val_accuracy: 0.4437\n",
      "Epoch 3/5\n",
      " - 20s - loss: 0.8568 - accuracy: 0.5747 - val_loss: 0.8474 - val_accuracy: 0.4901\n",
      "Epoch 4/5\n",
      " - 20s - loss: 0.8159 - accuracy: 0.5771 - val_loss: 0.8374 - val_accuracy: 0.6589\n",
      "Epoch 5/5\n",
      " - 20s - loss: 0.8098 - accuracy: 0.5951 - val_loss: 0.8136 - val_accuracy: 0.4735\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x7f0dba403f98>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 21
    }
   ],
   "source": [
    "print('Train model...')\n",
    "\n",
    "model.fit(x_train_pad, y_train, epochs=5, validation_data=(x_test_pad, y_test), verbose=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "\r 32/604 [>.............................] - ETA: 9s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 64/604 [==>...........................] - ETA: 9s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 96/604 [===>..........................] - ETA: 8s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r128/604 [=====>........................] - ETA: 7s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r160/604 [======>.......................] - ETA: 7s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r192/604 [========>.....................] - ETA: 6s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r224/604 [==========>...................] - ETA: 6s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/604 [===========>..................] - ETA: 5s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r288/604 [=============>................] - ETA: 5s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r320/604 [==============>...............] - ETA: 4s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r352/604 [================>.............] - ETA: 4s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/604 [==================>...........] - ETA: 3s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r416/604 [===================>..........] - ETA: 3s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r448/604 [=====================>........] - ETA: 2s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r480/604 [======================>.......] - ETA: 2s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r512/604 [========================>.....] - ETA: 1s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r544/604 [==========================>...] - ETA: 0s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r576/604 [===========================>..] - ETA: 0s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r604/604 [==============================] - 10s 17ms/step\n",
      "Test loss: 0.7390491129546766\nTest accuracy: 0.4867549538612366\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "loss, acc = model.evaluate(x_test_pad, y_test)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "LSTM(32),\n",
    "LSTM(64),\n",
    "split: 0.33,\n",
    "epochs: 5,\n",
    "dropout=r_dropout = 0.2,\n",
    "-> **loss: 0.74, acc: 0.52**\n",
    "\n",
    "LSTM(64),\n",
    "LSTM(64),\n",
    "split: 0.33,\n",
    "epochs: 20,\n",
    "dropout=r_dropout = 0.2,\n",
    "-> **loss: 0.64, acc: 0.49**\n",
    "\n",
    "LSTM(128),\n",
    "split: 0.33,\n",
    "epochs: 5,\n",
    "dropout=r_dropout = 0.2,\n",
    "-> **loss: 0.77, acc: 0.60**\n",
    "\n",
    "LSTM(32),\n",
    "split: 0.33,\n",
    "epochs: 5,\n",
    "dropout=r_dropout = 0.2,\n",
    "-> **loss: 0.81, acc: 0.64**\n",
    "\n",
    "LSTM(64),\n",
    "split: 0.33,\n",
    "epochs: 5,\n",
    "dropout=r_dropout = 0.2,\n",
    "-> **loss: 0.81, acc: 0.63**\n",
    "\n",
    "LSTM(64),\n",
    "split: 0.33,\n",
    "epochs: 5,\n",
    "dropout=r_dropout = 0.2,\n",
    "-> **loss: 0.75, acc: 0.57**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nLSTM(128),\\nsplit: 0.33,\\nepochs: 5,\\nloss: 1.2, acc: 0.41\\n------------\\n\\n'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 18
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}