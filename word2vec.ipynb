{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nhttps://towardsdatascience.com/machine-learning-word-embedding-sentiment-classification-using-keras-b83c28087456\\nhttp://ruder.io/deep-learning-nlp-best-practices/\\nhttps://skymind.ai/wiki/word2vec\\nhttps://medium.com/@ppasumarthi_69210/word-embeddings-in-keras-be6bb3092831\\nhttps://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526\\nhttps://medium.com/@dcameronsteinke/tf-idf-vs-word-embedding-a-comparison-and-code-tutorial-5ba341379ab0\\nhttps://medium.com/@b.terryjack/nlp-everything-about-word-embeddings-9ea21f51ccfe\\nhttps://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795\\n'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 1
    }
   ],
   "source": [
    "'''\n",
    "https://towardsdatascience.com/machine-learning-word-embedding-sentiment-classification-using-keras-b83c28087456\n",
    "http://ruder.io/deep-learning-nlp-best-practices/\n",
    "https://skymind.ai/wiki/word2vec\n",
    "https://medium.com/@ppasumarthi_69210/word-embeddings-in-keras-be6bb3092831\n",
    "https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526\n",
    "https://medium.com/@dcameronsteinke/tf-idf-vs-word-embedding-a-comparison-and-code-tutorial-5ba341379ab0\n",
    "https://medium.com/@b.terryjack/nlp-everything-about-word-embeddings-9ea21f51ccfe\n",
    "https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/konpyro/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /home/konpyro/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /home/konpyro/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package rslp to /home/konpyro/nltk_data...\n[nltk_data]   Package rslp is already up-to-date!\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 2
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('rslp')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# X: input, Y:output\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open('MoodyLyricsFull4Q.csv') as file:\n",
    "    reader = csv.reader(file, delimiter = ',')\n",
    "    count = 0\n",
    "    for row in reader:\n",
    "        if count > 0:\n",
    "            X.append(row[4])\n",
    "            Y.append(row[3])\n",
    "        count = count + 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#Y = np.concatenate(y_train, y_test)\n",
    "y_t = np.zeros((len(Y), 4))\n",
    "for i  in range(len(Y)):\n",
    "    if Y[i] == 'happy':\n",
    "        y_t[i][:] = [1, 0, 0, 0]\n",
    "    elif Y[i] == 'angry':\n",
    "        y_t[i][:] = [0, 1, 0, 0]\n",
    "    elif Y[i] == 'sad':\n",
    "        y_t[i][:] = [0, 0, 1, 0]\n",
    "    elif Y[i] == 'relaxed':\n",
    "        y_t[i][:] = [0, 0, 0, 1]\n",
    "Y = y_t"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[2, 101, 66, 7, 505, 2, 101, 2133, 9, 1965, 22, 2, 60, 2, 59, 1, 5455, 11, 5456, 38, 2, 298, 9, 419, 25, 43, 1, 46, 10, 1, 69, 11, 1, 93, 11, 2810, 60, 41, 2, 91, 9, 792, 5, 4775, 12, 1179, 4, 38, 3, 107, 11, 8, 9, 906, 199, 5, 3487, 3, 56, 162, 344, 5, 2, 89, 25, 23, 366, 30, 3, 166, 164, 377, 7, 26, 12, 1356, 370, 19, 201, 7, 252, 148, 7, 44, 66, 7, 269, 39, 145, 35, 60, 8, 3836, 21, 127, 221, 164, 21, 703, 3233, 10, 12, 247, 282, 2, 1130, 1, 1819, 3, 52, 724, 7, 2, 1429, 1, 414, 1820, 60, 2, 66, 8, 153, 153, 153, 613, 14, 1, 11148, 4198, 4, 8002, 2, 11149, 9, 614, 115, 2, 748, 9, 604, 4, 38, 3, 107, 11, 8, 9, 906, 199, 5, 3487, 3, 56, 162, 344, 5, 2, 89, 25, 23, 366, 30, 3, 166, 164, 377, 7, 26, 12, 1356, 370, 19, 201, 7, 252, 148, 7, 44, 66, 7, 269, 39, 145, 35, 60, 8, 3836, 21, 127, 221, 164, 21, 703, 3233, 10, 12, 247, 282]\n[    2   101    66     7   505     2   101  2133     9  1965    22     2\n    60     2    59     1  5455    11  5456    38     2   298     9   419\n    25    43     1    46    10     1    69    11     1    93    11  2810\n    60    41     2    91     9   792     5  4775    12  1179     4    38\n     3   107    11     8     9   906   199     5  3487     3    56   162\n   344     5     2    89    25    23   366    30     3   166   164   377\n     7    26    12  1356   370    19   201     7   252   148     7    44\n    66     7   269    39   145    35    60     8  3836    21   127   221\n   164    21   703  3233    10    12   247   282     2  1130     1  1819\n     3    52   724     7     2  1429     1   414  1820    60     2    66\n     8   153   153   153   613    14     1 11148  4198     4  8002     2\n 11149     9   614   115     2   748     9   604     4    38     3   107\n    11     8     9   906   199     5  3487     3    56   162   344     5\n     2    89    25    23   366    30     3   166   164   377     7    26\n    12  1356   370    19   201     7   252   148     7    44    66     7\n   269    39   145    35    60     8  3836    21   127   221   164    21\n   703  3233    10    12   247   282     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "total_lyrics = x_train + x_test\n",
    "t.fit_on_texts(total_lyrics)\n",
    "\n",
    "max_length = max([len(s.split()) for s in total_lyrics])\n",
    "vocab_size = len(t.word_index) + 1\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "x_train_tokens = t.texts_to_sequences(x_train)\n",
    "x_test_tokens = t.texts_to_sequences(x_test)\n",
    "\n",
    "x_train_pad = pad_sequences(x_train_tokens, padding='post')\n",
    "x_test_pad = pad_sequences(x_test_tokens, padding='post')\n",
    "\n",
    "print(x_train_tokens[1])\n",
    "print(x_train_pad[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lyric_lines = list()\n",
    "lines = X\n",
    "\n",
    "for line in lines:\n",
    "    tokens = word_tokenize(line)\n",
    "\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    lyric_lines.append(words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Vocabulary size: 20634\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec(sentences=lyric_lines, size=100, window=5, workers=4, min_count=1)\n",
    "\n",
    "words = list(model.wv.vocab)\n",
    "print(\"Vocabulary size: %d\" %len(words))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "[('mine', 0.9754360914230347),\n ('privacy', 0.971505880355835),\n ('kiss', 0.9699615240097046),\n ('true', 0.9668830633163452),\n ('feels', 0.965377151966095),\n ('darling', 0.9638769030570984),\n ('sure', 0.9632102251052856),\n ('thing', 0.9623692631721497),\n ('affection', 0.9617385864257812),\n ('please', 0.9617030024528503)]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 9
    }
   ],
   "source": [
    "model.wv.most_similar('love')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "\"'\\nfilename = 'lyric_embedding_word2vec'\\nmodel.wv.save(filename)\\n\\nimport os\\n\\nembeddings_index = {}\\nf = open(os.path.join('', 'lyric_embedding_word2vec.txt'), encoding='latin-1')\\nfor line in f:\\n  print(line)\\n  values = line.split()\\n  word = values[0]\\n  coefs = np.asarray(values[1:])\\n  embeddings_index[word] = coefs\\nf.close()\\n\""
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 10
    }
   ],
   "source": [
    "''''\n",
    "filename = 'lyric_embedding_word2vec'\n",
    "model.wv.save(filename)\n",
    "\n",
    "import os\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('', 'lyric_embedding_word2vec.txt'), encoding='latin-1')\n",
    "for line in f:\n",
    "  print(line)\n",
    "  values = line.split()\n",
    "  word = values[0]\n",
    "  coefs = np.asarray(values[1:])\n",
    "  embeddings_index[word] = coefs\n",
    "f.close()\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "my_dict = dict({})\n",
    "for idx, key in enumerate(model.wv.vocab):\n",
    "    my_dict[key] = model.wv[key]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# %store my_dict > dictionary.txt\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "embeddings_index = my_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Found 20634 unique tokens.\nShape of lyric tensor: (2000, 1015)\nShape of sentiment tensor; (2000, 4)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(lyric_lines)\n",
    "sequences = t.texts_to_sequences(lyric_lines)\n",
    "\n",
    "word_index = t.word_index\n",
    "print('Found %s unique tokens.' %len(word_index))\n",
    "\n",
    "review_pad = pad_sequences(sequences, maxlen=max_length)\n",
    "sentiment = np.asarray(Y)\n",
    "print('Shape of lyric tensor:', review_pad.shape)\n",
    "print('Shape of sentiment tensor;', sentiment.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "20635\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "num_words = len(word_index) + 1\n",
    "embeddings_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embeddings_matrix[i] = embedding_vector\n",
    "        \n",
    "print(num_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Biuld model...\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from keras.initializers import Constant\n",
    "\n",
    "print('Biuld model...')\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embeddings_matrix),\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "#model.add(LSTM(32, dropout=0.2, return_sequences=True, recurrent_dropout=0.2))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.33\n",
    "\n",
    "indices = np.arange(review_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "review_pad = review_pad[indices]\n",
    "sentiment = sentiment[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * review_pad.shape[0])\n",
    "\n",
    "x_train_pad = review_pad[:-num_validation_samples]\n",
    "y_train = sentiment[:-num_validation_samples]\n",
    "x_test_pad = review_pad[-num_validation_samples:]\n",
    "y_test = sentiment[-num_validation_samples:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Shape of x_train_pad tensor: (1340, 1015)\nShape of y_train tensor: (1340, 4)\nShape of x_test_pad tensor: (660, 1015)\nShape of y_test tensor: (660, 4)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print('Shape of x_train_pad tensor:', x_train_pad.shape)\n",
    "print('Shape of y_train tensor:', y_train.shape)\n",
    "print('Shape of x_test_pad tensor:', x_test_pad.shape)\n",
    "print('Shape of y_test tensor:', y_test.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Train model...\n",
      "Train on 1340 samples, validate on 660 samples\nEpoch 1/10\n",
      " - 34s - loss: 1.3806 - accuracy: 0.2963 - val_loss: 1.3456 - val_accuracy: 0.3379\n",
      "Epoch 2/10\n",
      " - 38s - loss: 1.3650 - accuracy: 0.3119 - val_loss: 1.3327 - val_accuracy: 0.3485\n",
      "Epoch 3/10\n",
      " - 34s - loss: 1.3432 - accuracy: 0.3455 - val_loss: 1.3318 - val_accuracy: 0.3561\n",
      "Epoch 4/10\n",
      " - 39s - loss: 1.3353 - accuracy: 0.3530 - val_loss: 1.3313 - val_accuracy: 0.3606\n",
      "Epoch 5/10\n",
      " - 37s - loss: 1.3208 - accuracy: 0.3634 - val_loss: 1.3324 - val_accuracy: 0.3682\n",
      "Epoch 6/10\n",
      " - 42s - loss: 1.3104 - accuracy: 0.3784 - val_loss: 1.3373 - val_accuracy: 0.3591\n",
      "Epoch 7/10\n",
      " - 39s - loss: 1.3062 - accuracy: 0.3657 - val_loss: 1.3169 - val_accuracy: 0.3697\n",
      "Epoch 8/10\n",
      " - 34s - loss: 1.3013 - accuracy: 0.3866 - val_loss: 1.3172 - val_accuracy: 0.3727\n",
      "Epoch 9/10\n",
      " - 34s - loss: 1.2914 - accuracy: 0.4007 - val_loss: 1.3049 - val_accuracy: 0.3561\n",
      "Epoch 10/10\n",
      " - 35s - loss: 1.2916 - accuracy: 0.3873 - val_loss: 1.3146 - val_accuracy: 0.3667\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x7fd8c1d9dfd0>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 19
    }
   ],
   "source": [
    "print('Train model...')\n",
    "\n",
    "model.fit(x_train_pad, y_train, epochs=10, validation_data=(x_test_pad, y_test), verbose=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "\r 32/660 [>.............................] - ETA: 5s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 64/660 [=>............................] - ETA: 6s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 96/660 [===>..........................] - ETA: 5s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r128/660 [====>.........................] - ETA: 5s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r160/660 [======>.......................] - ETA: 4s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r192/660 [=======>......................] - ETA: 4s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r224/660 [=========>....................] - ETA: 4s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/660 [==========>...................] - ETA: 4s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r288/660 [============>.................] - ETA: 3s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r320/660 [=============>................] - ETA: 3s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r352/660 [===============>..............] - ETA: 3s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/660 [================>.............] - ETA: 2s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r416/660 [=================>............] - ETA: 2s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r448/660 [===================>..........] - ETA: 2s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r480/660 [====================>.........] - ETA: 1s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r512/660 [======================>.......] - ETA: 1s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r544/660 [=======================>......] - ETA: 1s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r576/660 [=========================>....] - ETA: 0s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r608/660 [==========================>...] - ETA: 0s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r640/660 [============================>.] - ETA: 0s",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r660/660 [==============================] - 7s 11ms/step\n",
      "Test loss: 1.3145509553678107\nTest accuracy: 0.36666667461395264\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "loss, acc = model.evaluate(x_test_pad, y_test)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', acc)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "LSTM(64),\n",
    "split: 0.33,\n",
    "epochs: 5,\n",
    "dropout=r_dropout = 0.2,\n",
    "-> **loss: 1.31, acc: 0.36**\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}